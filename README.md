# Amazon Scout

<p align="center"><a href="https://amazon-scout.vercel.app/">amazon-scout.vercel.app</a></p>

<p align="center">
  <img src="docs/banner.png" alt="Qdrant Hybrid Search â€” phone â†’ Qdrant â†’ Hybrid â†’ Filters â†’ Rerank â†’ Results" width="85%"/>
</p>

<p align="center">
  <a href="https://img.shields.io/badge/Python-3.10%2B-blue.svg"><img src="https://img.shields.io/badge/Python-3.10%2B-blue.svg" /></a>
  <a href="https://img.shields.io/badge/Qdrant-Cloud-success"><img src="https://img.shields.io/badge/Qdrant-Cloud-success" /></a>
  <a href="https://img.shields.io/badge/GCP-Cloud%20Run%20%7C%20GKE-orange"><img src="https://img.shields.io/badge/GCP-Cloud%20Run%20%7C%20GKE-orange" /></a>
  <a href="https://img.shields.io/badge/License-MIT-green"><img src="https://img.shields.io/badge/License-MIT-green" /></a>
</p>

> **TL;DR** â€” Production-ready **hybrid search** on **2M Amazon reviews** (Dense + BM25) with **facets**, **reranking**, and **quantization** on **Qdrant Cloud (GCP)**.
> âœ… P\@10 â‰ˆ **99.8â€“100%** on our eval set Â· ğŸ’¾ **\~30% storage** saved with INT8 scalar quantization Â· ğŸŒ globally reachable API.

---

## ğŸ“Œ Features

* **Hybrid retrieval**: Dense + BM25 (sparse) fused for high recall
* **Facet filters**: price, rating, category, brand, verified purchase, date, etc.
* **Rerank stage**: plug-in *MiniCOIL* / *ColBERT* / *Cohere Rerank* (choose your flavor)
* **Quantization**: optional INT8 / PQ for lower storage & faster I/O
* **Cloud-native**: Qdrant Cloud on GCP + FastAPI microservice (ready for Cloud Run)
* **Batched ingestion**: stream JSONL/Parquet and index at scale
* **Eval kit**: Precision\@K / Recall\@K / NDCG
* **ğŸ¤– GPT Recommendations Agent (humanâ€‘review grounded)**: an LLM layer that **reads the retrieved human reviews** (not ads), summarizes pros/cons, and **recommends products** with **quoted evidence** from real user text. No sponsored placement, no brand biasâ€”**recommendations are grounded in what people actually said.**

---

## ğŸ§­ Architecture

The system is intentionally simple so itâ€™s easy to operate and extend.

> **Why this order?** Hybrid gets you **recall**. Filters keep it **relevant**. Rerank polishes **topâ€‘K**. Kid-simple: *find a big pile â†’ keep only what you asked for â†’ sort the best on top.*

### + Agent layer for trusted recommendations

**What the agent does**

* Consumes the **topâ€‘K** retrieved review payloads (title, text, stars, price, brand, etc.).
* Produces **concise, humanâ€‘readable recommendations** (e.g., Top 3) with **grounded snippets** from real reviews.
* **Cites** each reason with `review_id` + quoted text so readers can verify.
* Enforces **â€œfactsâ€‘fromâ€‘reviewsâ€‘onlyâ€**: avoids hallucinations and marketing copy.

---

## ğŸš€ Quickstart (No-Code Overview)

1. **Create a Python environment** and install: Qdrant Client (with FastEmbed), FastAPI, uvicorn, orjson, pydantic. Optionally add sentence-transformers (for rerankers) and pandas/pyarrow (for Parquet).
2. **Configure Qdrant Cloud**: set your endpoint URL and API key as environment variables. Use Qdrant Cloud (GCP) project/cluster.
3. **Create the collection** with two vector families: a 384â€‘dim **dense** vector (cosine) and a **bm25** sparse vector. Define typed payload fields for facets (price, stars, category, brand, verified, dateâ€¦).
4. **Ingest data** from JSONL/Parquet: embed `title + text` for dense and sparse, upsert with payload. Use batching (e.g., 512) for throughput.
5. **(Optional) Quantize** dense vectors (INT8 or PQ) after validating quality. Expect \~30% storage savings with scalar INT8.
6. **Serve a search API** (e.g., FastAPI) that performs **Hybrid â†’ Filters â†’ Rerank** and returns review cards. Deploy on **Cloud Run**.

---

## ğŸ¤– GPT Recommendations Agent (Humanâ€‘Trusted Reviews â†’ Product Picks)

**Mission:** *Recommend products using only what real people wrote.* Not ads, not manufacturer blurbsâ€”**ground everything** in the retrieved review texts.

### What you get

* **Summarized pros/cons** per product with **evidence quotes** from actual reviews
* **Topâ€‘N picks** tailored to the query and facet filters
* Optional **Q\&A** over reviews (e.g., â€œIs this noisy?â€, â€œFits wide feet?â€) with citations

### Why trust this?

* **No sponsored placement** and **no brand bias** â€” everything flows from what reviewers said.
* **Verifiable**: Every claim includes a `review_id` + **quoted snippet** so readers can check the source.
* **Guardrails**: The agent is instructed to avoid speculation or unverifiable claims.

### How to enable

* Provide credentials for your preferred LLM provider (OpenAI/Anthropic/Vertex/other).
* Keep temperature low (â‰ˆ0.1â€“0.3) for stable, auditâ€‘friendly outputs.
* The agent consumes only the retrieved review payloads and returns concise JSON with `{id, title, why, quotes[]}` â€” where `quotes[]` are short verbatim snippets with `review_id`s.

### Endpoints (conceptual)

* **`/search`** â€” Hybrid search with facet filters; returns candidate review cards.
* **`/recommend`** â€” Runs `/search`, then the GPT agent to produce Topâ€‘N picks with quotes.
* **`/ask`** â€” Freeâ€‘form Q\&A over the retrieved review set; answers include citations.

---

## ğŸ“Š Evaluation

* Report **Precision\@K / Recall\@K / NDCG** on a curated test set. We observed **P\@10 â‰ˆ 99.8â€“100%** after reranking (dependent on model choices and label quality).
* Agent-side health: track **evidence coverage** (% of recommendation sentences backed by quotes) and **contradiction rate** (whether quotes conflict with the claim).

---

## ğŸ§© Project Layout (Conceptual)

* `api.py` â€” FastAPI service surface (search + recommend + ask)
* `data/reviews.jsonl` â€” Input dataset (id/title/text/stars/price/...)
* `docs/` â€” Visuals (banner, diagrams)
* `scripts/ingest.py` â€” Batch ingestion/indexing logic
* `requirements.txt` â€” Python deps
* `README.md` â€” This file

---

## ğŸ§± Design notes (teammate truth)

* **Hybrid first** for **recall**, **filters second** for **precision**, **rerank last** for **quality**. That order is correct â€” *kid-simple pipeline*.
* **Agent** only reads **what was retrieved**; it **quotes** evidence for every claim and avoids marketing language.
* **Quantize** only after youâ€™ve validated quality (always run A/B on P\@K).
* Keep the **payload** skinny and **facets** typed (numeric for ranges, enums for categories) for fast filtering.
* Oversample (e.g., `top_k Ã— 5`) **before** rerank; otherwise the reranker canâ€™t help enough.

---

## ğŸ“ License

MIT â€” do good things. If this saves you time, â­ the repo and tell a friend.

---

## ğŸ™Œ Acknowledgements

* Built with **Qdrant Cloud** on **Google Cloud Platform**
* Dense + Sparse via **FastEmbed**; bring your own models if you prefer
* Optional rerankers: **MiniCOIL / ColBERT / Cohere Rerank**
* GPT Recommendations Agent: generic LLM interface (plug your provider of choice).
